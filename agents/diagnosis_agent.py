import httpx
from config.settings import settings

class DiagnosisAgent:
    """Agente de raciocÃ­nio clÃ­nico veterinÃ¡rio"""
    
    def __init__(self):
        self.client = httpx.Client(timeout=60.0)  # Timeout de 60s por chamada
    
    def generate_diagnosis(
        self,
        animal_info: dict,
        symptoms: str,
        visual_analysis: str,
        knowledge: dict,
        model: str = None  # Modelo opcional
    ) -> dict:
        """
        Gera diagnÃ³stico diferencial baseado em todas as informaÃ§Ãµes
        """
        
        # Formatar conhecimento
        doc_context = "\n".join([
            f"[Documento: {d['source']}]\n{d['content']}"
            for d in knowledge.get("local_documents", [])[:3]
        ])
        
        web_context = knowledge.get("web_search", "")
        
        prompt = f"""Ã‰s um veterinÃ¡rio experiente a realizar um diagnÃ³stico diferencial.

## INFORMAÃ‡ÃƒO DO CASO

### Dados do Animal:
- EspÃ©cie: {animal_info.get('especie')}
- RaÃ§a: {animal_info.get('raca')}
- Idade: {animal_info.get('idade')}
- Peso: {animal_info.get('peso')}
- HistÃ³rico mÃ©dico: {animal_info.get('historico', 'NÃ£o disponÃ­vel')}

### Sintomas Reportados pelo Tutor:
{symptoms}

### AnÃ¡lise Visual das Imagens:
{visual_analysis}

### InformaÃ§Ã£o de ReferÃªncia (Literatura e Web):
{doc_context}

{web_context}

---

## TAREFA

Com base em toda a informaÃ§Ã£o, fornece:

### 1. DIAGNÃ“STICOS DIFERENCIAIS
Lista os 3-5 diagnÃ³sticos mais provÃ¡veis, ordenados por probabilidade:
- Para cada um: nome, probabilidade estimada (%), justificaÃ§Ã£o

### 2. EXAMES RECOMENDADOS
Que exames/testes confirmariam o diagnÃ³stico:
- AnÃ¡lises laboratoriais
- Imagiologia
- Outros testes

### 3. TRATAMENTO INICIAL
SugestÃµes de tratamento/manejo enquanto nÃ£o hÃ¡ diagnÃ³stico definitivo:
- Cuidados imediatos
- MedicaÃ§Ã£o sintomÃ¡tica (se aplicÃ¡vel)
- O que NÃƒO fazer

### 4. NÃVEL DE URGÃŠNCIA
Classifica: ðŸŸ¢ Rotina | ðŸŸ¡ Consulta em 24-48h | ðŸ”´ Urgente | âš« EmergÃªncia

### 5. PRÃ“XIMOS PASSOS
RecomendaÃ§Ãµes claras para o tutor

### 6. DISCLAIMER
Lembra que isto Ã© uma orientaÃ§Ã£o e nÃ£o substitui consulta presencial.

---
Raciocina passo a passo antes de concluir."""

        # Tentar mÃºltiplos providers em sequÃªncia
        model_used = "unknown"
        response = None
        
        # Se um modelo especÃ­fico foi passado, usar diretamente
        if model:
            try:
                model_short = model.split("/")[-1].split(":")[0]
                print(f"   Usando modelo selecionado: {model_short}...")
                
                # Determinar provider pelo formato do modelo
                if model.startswith("mistral"):
                    response = self._call_mistral(prompt, model)
                    model_used = model_short
                elif model.startswith("gemini"):
                    response = self._call_gemini(prompt)
                    model_used = model_short
                else:
                    # Assumir OpenRouter
                    response = self._call_openrouter(prompt, model)
                    model_used = model_short
                    
            except Exception as e:
                print(f"   Modelo selecionado falhou: {e}")
                # Continuar com fallback
        
        # Fallback: tentar mÃºltiplos providers
        if response is None:
            # Lista de modelos OpenRouter para tentar (gratuitos)
            openrouter_models = [
                (settings.LLM_OPENROUTER_1, "grok-4.1-fast"),
                (settings.LLM_OPENROUTER_3, "gemma-3-27b"),
                (settings.LLM_OPENROUTER_2, "deepseek-r1-chimera"),
                (settings.LLM_OPENROUTER_4, "glm-4.5-air"),
            ]
            
            # 1. Tentar modelos OpenRouter
            for llm_model, name in openrouter_models:
                try:
                    print(f"   Tentando {name}...")
                    response = self._call_openrouter(prompt, llm_model)
                    model_used = name
                    break
                except Exception as e:
                    print(f"   {name} falhou: {e}")
                    continue
        
        # 2. Se OpenRouter falhou, tentar Mistral
        if response is None:
            try:
                print("   Tentando Mistral...")
                response = self._call_mistral(prompt)
                model_used = "mistral-small"
            except Exception as e2:
                print(f"   Mistral falhou: {e2}")
                
                # 3. Tentar Gemini
                try:
                    print("   Tentando Gemini...")
                    response = self._call_gemini(prompt)
                    model_used = "gemini"
                except Exception as e3:
                    print(f"   Gemini falhou: {e3}")
                    response = "âŒ NÃ£o foi possÃ­vel gerar diagnÃ³stico. Verifique as API keys."
                    model_used = "none"
        
        return {
            "diagnosis_report": response,
            "model_used": model_used
        }
    
    def _call_openrouter(self, prompt: str, model: str = None) -> str:
        """Chama modelo via OpenRouter (gratuito)"""
        model = model or settings.LLM_OPENROUTER_1  # tngtech/deepseek-r1t-chimera:free
        response = self.client.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {settings.OPENROUTER_API_KEY}",
                "Content-Type": "application/json",
                "HTTP-Referer": "http://localhost",
                "X-Title": "VetDiagnosis"
            },
            json={
                "model": model,
                "messages": [
                    {
                        "role": "system",
                        "content": "Ã‰s um veterinÃ¡rio especialista em diagnÃ³stico."
                    },
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": 4000,
                "temperature": 0.2
            }
        )
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    
    def _call_mistral(self, prompt: str, model: str = None) -> str:
        """Backup: Mistral AI"""
        mistral_model = model or settings.LLM_BACKUP  # mistral-small-latest
        response = self.client.post(
            "https://api.mistral.ai/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {settings.MISTRAL_API_KEY}",
                "Content-Type": "application/json"
            },
            json={
                "model": mistral_model,
                "messages": [
                    {
                        "role": "system",
                        "content": "Ã‰s um veterinÃ¡rio especialista em diagnÃ³stico."
                    },
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": 4000,
                "temperature": 0.2
            }
        )
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    
    def _call_gemini(self, prompt: str) -> str:
        """Backup: Gemini"""
        import google.generativeai as genai
        
        genai.configure(api_key=settings.GOOGLE_API_KEY)
        model = genai.GenerativeModel(settings.VLM_MODEL) 
        
        response = model.generate_content(prompt)
        return response.text